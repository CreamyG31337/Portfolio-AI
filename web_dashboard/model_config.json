{
    "default_config": {
        "num_ctx": 4096,
        "num_predict": 2048,
        "temperature": 0.1,
        "note": "Fallback settings if model not found"
    },
    "models": {
        "mistral-nemo:12b": {
            "num_ctx": 16384,
            "num_predict": 4096,
            "temperature": 0.1,
            "desc": "Best All-Rounder. High context, fits comfortably in VRAM."
        },
        "llama3.1:8b": {
            "num_ctx": 32768,
            "num_predict": 8192,
            "temperature": 0.1,
            "desc": "Safest choice. Low VRAM usage allows massive context."
        },
        "deepseek-r1:14b": {
            "num_ctx": 8192,
            "num_predict": 2048,
            "temperature": 0.6,
            "desc": "Reasoning Model. High Temp required. Good for deep logic."
        },
        "phi4-reasoning:14b": {
            "num_ctx": 8192,
            "num_predict": 2048,
            "temperature": 0.6,
            "desc": "Reasoning Model. High Temp required."
        },
        "deepseek-r1:8b": {
            "num_ctx": 16384,
            "num_predict": 4096,
            "temperature": 0.6,
            "desc": "Faster/Lighter reasoning model."
        },
        "qwen3:14b": {
            "num_ctx": 8192,
            "num_predict": 2048,
            "temperature": 0.2,
            "desc": "Excellent at math and numbers."
        },
        "gemma3:12b": {
            "num_ctx": 8192,
            "num_predict": 2048,
            "temperature": 0.1,
            "desc": "Strong alternative to Mistral."
        },
        "granite3.3:8b": {
            "num_ctx": 16384,
            "num_predict": 4096,
            "temperature": 0.1,
            "desc": "Enterprise grade, very stable for RAG/Docs."
        },
        "phi4:14b": {
            "num_ctx": 8192,
            "num_predict": 2048,
            "temperature": 0.1,
            "desc": "Strong logic, non-reasoning variant."
        },
        "dolphin3:8b": {
            "num_ctx": 16384,
            "num_predict": 4096,
            "temperature": 0.3,
            "desc": "Uncensored. Good if other models refuse to analyze 'risky' stocks."
        },
        "exaone-deep:7.8b": {
            "num_ctx": 16384,
            "num_predict": 4096,
            "temperature": 0.1,
            "desc": "Lightweight, good for simple tasks."
        },
        "qwen3-coder:latest": {
            "num_ctx": 16384,
            "num_predict": 4096,
            "temperature": 0.1,
            "desc": "Optimized for code, but accurate with structured data/JSON.",
            "hidden": true
        },
        "gpt-oss:20b": {
            "num_ctx": 4096,
            "num_predict": 1024,
            "temperature": 0.2,
            "desc": "Heavy. Low context required to fit 16GB VRAM."
        },
        "qwen3:30b-a3b": {
            "num_ctx": 2048,
            "num_predict": 512,
            "temperature": 0.2,
            "desc": "Very Heavy. Minimal context only or will OOM/Swap."
        },
        "dolphin-mixtral:8x7b": {
            "num_ctx": 2048,
            "num_predict": 512,
            "temperature": 0.3,
            "desc": "Too big for GPU. Will be slow (CPU offload)."
        },
        "llama3.2:3b": {
            "num_ctx": 8192,
            "num_predict": 512,
            "temperature": 0.1,
            "desc": "Small, fast model optimized for summarization tasks"
        },
        "nomic-embed-text:latest": {
            "num_ctx": 2048,
            "num_predict": 512,
            "temperature": 0.1,
            "desc": "Embedding model for text similarity and RAG",
            "hidden": true
        },
        "glm-4.7": {
            "provider": "zhipu",
            "num_ctx": 128000,
            "temperature": 0.1,
            "max_tokens": 4096,
            "desc": "Zhipu GLM-4.7 via Z.AI. Best quality."
        },
        "glm-4.5-air": {
            "provider": "zhipu",
            "num_ctx": 128000,
            "temperature": 0.1,
            "max_tokens": 4096,
            "desc": "Zhipu GLM-4.5 Air via Z.AI. Faster, lighter."
        },
        "gemini-2.5-flash": {
            "provider": "webai",
            "num_ctx": 1048576,
            "temperature": 0.1,
            "max_tokens": 8192,
            "desc": "Fast responses with 1M token context"
        },
        "gemini-2.5-pro": {
            "provider": "webai",
            "num_ctx": 2097152,
            "temperature": 0.1,
            "max_tokens": 8192,
            "desc": "Advanced reasoning with 2M token context"
        },
        "gemini-3.0-pro": {
            "provider": "webai",
            "num_ctx": 2097152,
            "temperature": 0.1,
            "max_tokens": 8192,
            "desc": "Latest model with 2M token context"
        }
    }
}