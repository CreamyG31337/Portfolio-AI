# Streamlit Dashboard Build and Deploy Pipeline
#
# This pipeline builds the Streamlit dashboard Docker image and deploys it to the server.
# It uses Docker socket mounting to build directly on the server (no SSH needed).
#
# Build Process:
# 1. Build Docker image from web_dashboard/Dockerfile
# 2. Tag image with commit SHA for version tracking
# 3. Stop and remove existing container (if running)
# 4. Deploy new container with environment variables from Woodpecker secrets
#
# Available Secrets (configure in Woodpecker - use lowercase names):
# - supabase_url: Your Supabase project URL
# - supabase_publishable_key: For user authentication
# - supabase_secret_key: For admin scripts and debug operations
# - app_domain: Your application domain (e.g., "ai-trading.drifting.space")
#               Used for magic links, password resets, and cookie domain settings
# - research_database_url: (optional) Postgres connection string for research articles
#                          Format: postgresql://user:password@host:port/database
#                          For Docker container connecting to host Postgres: use host.docker.internal
#                          Example: postgresql://postgres:password@host.docker.internal:5432/trading_db
# - supabase_database_url: (required) PostgreSQL connection string for Supabase database
#                          Used by APScheduler's SQLAlchemyJobStore for persistent job storage
#                          IMPORTANT: Use Connection Pooler (Supavisor) for IPv4 support
#                          Format: postgresql://postgres.[project-ref]:[password]@aws-0-[region].pooler.supabase.com:6543/postgres
#                          Get from: Supabase Dashboard -> Settings -> Database -> Connection Pooling -> Transaction Mode
#                          Example: postgresql://postgres.abc123:YOUR-PASSWORD@aws-0-us-east-1.pooler.supabase.com:6543/postgres
#                          Note: Port 6543 (pooler) instead of 5432 (direct connection)
# - fmp_api_key: (optional) Financial Modeling Prep API key for congress trading module
#                Get from: https://site.financialmodelingprep.com/developer/docs/
#                Required for congress_trades scheduled job
# - webai_cookies_json: (optional) JSON string of cookies for WebAI Pro web-based AI
#                       Format: {"__Secure-1PSID":"value","__Secure-1PSIDTS":"value"}
#                       Extract using: python web_dashboard/extract_ai_cookies.py --browser manual
#                       Required for WebAI Pro model in AI Assistant
#                       Note: Used only for initial cookie setup. A cookie refresher sidecar container
#                       automatically refreshes cookies and writes them to /shared/cookies/webai_cookies.json
#                       The main app reads cookies from the shared volume, not from this environment variable
# - ai_service_web_url: (optional) Web interface URL for AI service (obfuscated)
#                       Only needed if default fallback URL doesn't work
#                       Set via environment variable when generating ai_service.keys.json
# - ollama_base_url: (optional) Ollama API URL (default: http://host.docker.internal:11434)
# - ollama_model: (optional) Default AI model (default: mistral-nemo:12b)
# - ollama_enabled: (optional) Enable AI assistant (default: true)
#
# Note: Secrets are loaded using from_secret in the environment section
# Note: Repository must be marked as "Trusted" in Woodpecker settings to use volumes

steps:
  build-and-deploy:
    image: docker:24
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /home/lance/ai-trading-www:/deploy_target
    environment:
      TZ: America/Vancouver
      SUPABASE_URL:
        from_secret: supabase_url
      SUPABASE_PUBLISHABLE_KEY:
        from_secret: supabase_publishable_key
      SUPABASE_SECRET_KEY:
        from_secret: supabase_secret_key
      APP_DOMAIN:
        from_secret: app_domain
      RESEARCH_DATABASE_URL:
        from_secret: research_database_url
      SUPABASE_DATABASE_URL:
        from_secret: supabase_database_url
      FMP_API_KEY:
        from_secret: fmp_api_key
      WEBAI_COOKIES_JSON:
        from_secret: webai_cookies_json
      AI_SERVICE_WEB_URL:
        from_secret: ai_service_web_url
    commands:
      # Set timezone (Pacific Time - handles PST/PDT automatically)
      - ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
      
      # Format build timestamp in UTC (will be converted to user's timezone in Python)
      # CI_PIPELINE_STARTED is Unix timestamp, format in UTC
      - |
        if [ -n "$CI_PIPELINE_STARTED" ]; then
          export BUILD_TIMESTAMP=$(date -u -d "@$CI_PIPELINE_STARTED" "+%Y-%m-%d %H:%M UTC" 2>/dev/null || date -u -r "$CI_PIPELINE_STARTED" "+%Y-%m-%d %H:%M UTC" 2>/dev/null || date -u "+%Y-%m-%d %H:%M UTC")
        else
          export BUILD_TIMESTAMP=$(date -u "+%Y-%m-%d %H:%M UTC")
        fi
      
      # Create all required directories once at the start
      - mkdir -p /home/lance/trading-dashboard-logs /shared/cookies /deploy_target/frontend
      - chmod 777 /home/lance/trading-dashboard-logs /shared/cookies
      
      # Build shared base image first (required by Streamlit and Flask images)
      # Using BuildKit for better caching and --cache-from for cross-build cache sharing
      - |
        export DOCKER_BUILDKIT=1
        export COMPOSE_DOCKER_CLI_BUILD=1
        echo "Building shared base image with common Python dependencies..."
        # Use previous base image as cache source if available (from previous CI runs)
        # This enables cache sharing between builds even when layers would normally be invalidated
        docker build --progress=plain \
          --cache-from trading-dashboard-base:latest \
          -f web_dashboard/Dockerfile.base \
          -t trading-dashboard-base:latest . || {
          echo "❌ ERROR: Base image build failed"
          exit 1
        }
        echo "✅ Base image built successfully"
      
      # Build all Docker images in parallel for faster deployment
      # Note: Tailwind CSS is now built inside Dockerfiles using multi-stage builds
      # This leverages Docker layer caching and eliminates the need to install Node.js in CI
      # Using BuildKit for better caching and parallel builds
      - |
        set +e  # Don't exit on error - we'll handle errors manually
        export DOCKER_BUILDKIT=1
        export COMPOSE_DOCKER_CLI_BUILD=1
        echo "Building Docker images in parallel with BuildKit..."
        echo "This may take a few minutes on first build, but subsequent builds will be faster with caching..."
        
        # Build images in parallel with proper error handling
        echo "Starting parallel builds..."
        
        # Create temporary files to capture build outputs and exit codes
        STREAMLIT_LOG=$(mktemp)
        FLASK_LOG=$(mktemp)
        COOKIE_LOG=$(mktemp)
        STREAMLIT_EXIT_FILE="${STREAMLIT_LOG}.exit"
        FLASK_EXIT_FILE="${FLASK_LOG}.exit"
        COOKIE_EXIT_FILE="${COOKIE_LOG}.exit"
        
        # Debug: Verify variables are set correctly
        echo "Debug: STREAMLIT_EXIT_FILE=$STREAMLIT_EXIT_FILE"
        echo "Debug: FLASK_EXIT_FILE=$FLASK_EXIT_FILE"
        echo "Debug: COOKIE_EXIT_FILE=$COOKIE_EXIT_FILE"
        
        echo "Log files: STREAMLIT=$STREAMLIT_LOG FLASK=$FLASK_LOG COOKIE=$COOKIE_LOG"
        
        # Build Streamlit image (capture both stdout and stderr)
        # Use previous images as cache sources for better cache hits
        (docker build --progress=plain \
          --build-arg CACHEBUST=${CI_COMMIT_SHA} \
          --cache-from trading-dashboard-base:latest \
          --cache-from trading-dashboard:latest \
          -f web_dashboard/Dockerfile \
          -t trading-dashboard:latest . > "$STREAMLIT_LOG" 2>&1; echo $? > "$STREAMLIT_EXIT_FILE") &
        STREAMLIT_PID=$!
        
        # Build Flask image (capture both stdout and stderr)
        # Use previous images as cache sources for better cache hits
        (docker build --progress=plain \
          --build-arg CACHEBUST=${CI_COMMIT_SHA} \
          --cache-from trading-dashboard-base:latest \
          --cache-from trading-dashboard-flask:latest \
          -f web_dashboard/Dockerfile.flask \
          -t trading-dashboard-flask:latest . > "$FLASK_LOG" 2>&1; echo $? > "$FLASK_EXIT_FILE") &
        FLASK_PID=$!
        
        # Build Cookie refresher image (capture both stdout and stderr)
        (docker build --progress=plain -f web_dashboard/Dockerfile.cookie-refresher -t cookie-refresher:latest . > "$COOKIE_LOG" 2>&1; echo $? > "$COOKIE_EXIT_FILE") &
        COOKIE_PID=$!
        
        echo "Waiting for builds to complete (PIDs: Streamlit=$STREAMLIT_PID Flask=$FLASK_PID Cookie=$COOKIE_PID)..."
        
        # Wait for all builds to complete and capture exit codes
        wait $STREAMLIT_PID
        STREAMLIT_WAIT_EXIT=$?
        wait $FLASK_PID
        FLASK_WAIT_EXIT=$?
        wait $COOKIE_PID
        COOKIE_WAIT_EXIT=$?
        
        echo "Builds completed. Wait exit codes: Streamlit=$STREAMLIT_WAIT_EXIT Flask=$FLASK_WAIT_EXIT Cookie=$COOKIE_WAIT_EXIT"
        
        # Read exit codes from files (more reliable than wait exit codes)
        STREAMLIT_EXIT=$(cat "$STREAMLIT_EXIT_FILE" 2>/dev/null || echo "1")
        FLASK_EXIT=$(cat "$FLASK_EXIT_FILE" 2>/dev/null || echo "1")
        COOKIE_EXIT=$(cat "$COOKIE_EXIT_FILE" 2>/dev/null || echo "1")
        
        echo "Build exit codes from files: Streamlit=$STREAMLIT_EXIT Flask=$FLASK_EXIT Cookie=$COOKIE_EXIT"
        
        # Check results and show errors
        BUILD_ERRORS=0
        if [ "$STREAMLIT_EXIT" != "0" ]; then
          echo "❌ ERROR: Streamlit build failed (exit code: $STREAMLIT_EXIT)"
          echo "=== Full Streamlit build output ==="
          cat "$STREAMLIT_LOG" || echo "No log available"
          echo "=== End Streamlit build output ==="
          BUILD_ERRORS=$((BUILD_ERRORS + 1))
        else
          echo "✅ Streamlit build succeeded"
        fi
        
        if [ "$FLASK_EXIT" != "0" ]; then
          echo "❌ ERROR: Flask build failed (exit code: $FLASK_EXIT)"
          echo "=== Full Flask build output ==="
          cat "$FLASK_LOG" || echo "No log available"
          echo "=== End Flask build output ==="
          BUILD_ERRORS=$((BUILD_ERRORS + 1))
        else
          echo "✅ Flask build succeeded"
        fi
        
        if [ "$COOKIE_EXIT" != "0" ]; then
          echo "❌ ERROR: Cookie refresher build failed (exit code: $COOKIE_EXIT)"
          echo "=== Full Cookie refresher build output ==="
          cat "$COOKIE_LOG" || echo "No log available"
          echo "=== End Cookie refresher build output ==="
          BUILD_ERRORS=$((BUILD_ERRORS + 1))
        else
          echo "✅ Cookie refresher build succeeded"
        fi
        
        # Clean up log files (but keep them if builds failed for debugging)
        if [ $BUILD_ERRORS -eq 0 ]; then
          rm -f "$STREAMLIT_LOG" "$STREAMLIT_EXIT_FILE" "$FLASK_LOG" "$FLASK_EXIT_FILE" "$COOKIE_LOG" "$COOKIE_EXIT_FILE"
        else
          echo "⚠️  Keeping log files for debugging: $STREAMLIT_LOG $FLASK_LOG $COOKIE_LOG"
        fi
        
        if [ $BUILD_ERRORS -gt 0 ]; then
          echo "❌ ERROR: $BUILD_ERRORS build(s) failed"
          echo "Checking which images were built successfully..."
          docker images | grep -E "(trading-dashboard|cookie-refresher)" || echo "No images found"
          exit 1
        fi
        
        # Images were already verified above, but do a final check before tagging
        echo "Final verification before tagging..."
        if ! docker images trading-dashboard:latest --format "{{.Repository}}:{{.Tag}}" | grep -q "trading-dashboard:latest" 2>/dev/null; then
          echo "❌ Streamlit image not found after build"
          exit 1
        fi
        if ! docker images trading-dashboard-flask:latest --format "{{.Repository}}:{{.Tag}}" | grep -q "trading-dashboard-flask:latest" 2>/dev/null; then
          echo "❌ Flask image not found after build"
          exit 1
        fi
        if ! docker images cookie-refresher:latest --format "{{.Repository}}:{{.Tag}}" | grep -q "cookie-refresher:latest" 2>/dev/null; then
          echo "❌ Cookie refresher image not found after build"
          exit 1
        fi
        echo "✅ All images verified"
        
        echo "✅ All builds completed successfully"
        echo "Tagging images with commit SHA..."
        docker tag trading-dashboard:latest trading-dashboard:${CI_COMMIT_SHA} || { echo "❌ Failed to tag Streamlit image"; exit 1; }
        docker tag trading-dashboard-flask:latest trading-dashboard-flask:${CI_COMMIT_SHA} || { echo "❌ Failed to tag Flask image"; exit 1; }
        docker tag cookie-refresher:latest cookie-refresher:${CI_COMMIT_SHA} || { echo "❌ Failed to tag cookie-refresher image"; exit 1; }
        echo "✅ All images built and tagged"
      
      # Stop and remove existing containers if they exist (parallel stops for speed)
      - |
        echo "Stopping existing containers in parallel..."
        (docker stop trading-dashboard 2>/dev/null || true) &
        (docker stop trading-dashboard-flask 2>/dev/null || true) &
        wait
        echo "Removing stopped containers..."
        (docker rm trading-dashboard 2>/dev/null || true) &
        (docker rm trading-dashboard-flask 2>/dev/null || true) &
        wait
        echo "✅ Containers stopped and removed (some may not have existed, which is fine)"
      
      # Initialize shared cookies directory and file (before main container starts)
      - |
        if [ -n "$WEBAI_COOKIES_JSON" ]; then
          echo "$WEBAI_COOKIES_JSON" > /shared/cookies/webai_cookies.json
          chmod 644 /shared/cookies/webai_cookies.json
          echo "✅ Initialized cookie file from Woodpecker secret (before main container starts)"
          echo "   File location: /shared/cookies/webai_cookies.json"
          echo "   File size: $(wc -c < /shared/cookies/webai_cookies.json) bytes"
          echo "   File exists: $(test -f /shared/cookies/webai_cookies.json && echo 'YES' || echo 'NO')"
        else
          echo "⚠️  WARNING: WEBAI_COOKIES_JSON not set. Cookie file will be empty."
        fi
      
      # Verify secrets are set
      - |
        if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_PUBLISHABLE_KEY" ] || [ -z "$SUPABASE_SECRET_KEY" ] || [ -z "$SUPABASE_DATABASE_URL" ]; then
          echo "❌ ERROR: Missing required secrets"
          echo "Make sure supabase_url, supabase_publishable_key, supabase_secret_key, and supabase_database_url are set in Woodpecker"
          exit 1
        fi
      
      # Deploy new container (single line to avoid parsing issues)
      # Added --add-host to allow connecting to Ollama on the host machine via host.docker.internal (Linux support)
      # NOTE: On Windows/Mac, host.docker.internal works natively. On Linux, you MUST add "--add-host=host.docker.internal:host-gateway"
      # IPv6 support: Containers will use IPv6 if Docker daemon has IPv6 enabled (see DOCKER_IPV6_SETUP.md)
      # Ollama settings can be overridden via Woodpecker secrets or environment variables
      # RESEARCH_DATABASE_URL is optional - only set if research articles storage is needed
      # FMP_API_KEY is optional - only set if congress trading module is needed
      - |
        # Build base docker run command
        DOCKER_CMD="docker run -d --name trading-dashboard --restart unless-stopped -p 8501:8501 --network bridge-ipv6 --add-host=host.docker.internal:host-gateway"
        # Mount application logs directory (host logs -> container logs)
        DOCKER_CMD="$DOCKER_CMD -v /home/lance/trading-dashboard-logs:/app/web_dashboard/logs"
        # Mount Ollama logs directory (host logs -> container logs/server)
        DOCKER_CMD="$DOCKER_CMD -v /home/lance/ollama-logs:/app/web_dashboard/logs/server"
        # Mount shared cookies directory (for cookie refresher sidecar)
        DOCKER_CMD="$DOCKER_CMD -v /shared/cookies:/shared/cookies"
        DOCKER_CMD="$DOCKER_CMD -e SUPABASE_URL=\"$SUPABASE_URL\""
        DOCKER_CMD="$DOCKER_CMD -e SUPABASE_PUBLISHABLE_KEY=\"$SUPABASE_PUBLISHABLE_KEY\""
        DOCKER_CMD="$DOCKER_CMD -e SUPABASE_SECRET_KEY=\"$SUPABASE_SECRET_KEY\""
        DOCKER_CMD="$DOCKER_CMD -e BUILD_TIMESTAMP=\"$BUILD_TIMESTAMP\""
        
        # Add APP_DOMAIN (required for auth callbacks and cookie domain)
        [ -n "$APP_DOMAIN" ] && DOCKER_CMD="$DOCKER_CMD -e APP_DOMAIN=\"$APP_DOMAIN\""
        
        # Add optional environment variables if set
        [ -n "$RESEARCH_DATABASE_URL" ] && DOCKER_CMD="$DOCKER_CMD -e RESEARCH_DATABASE_URL=\"$RESEARCH_DATABASE_URL\""
        [ -n "$SUPABASE_DATABASE_URL" ] && DOCKER_CMD="$DOCKER_CMD -e SUPABASE_DATABASE_URL=\"$SUPABASE_DATABASE_URL\""
        [ -n "$FMP_API_KEY" ] && DOCKER_CMD="$DOCKER_CMD -e FMP_API_KEY=\"$FMP_API_KEY\""
        
        # Add WebAI cookies as environment variable (fallback if shared volume file not available)
        # The cookie refresher sidecar writes to /shared/cookies/webai_cookies.json
        # Main app checks shared volume first, then falls back to WEBAI_COOKIES_JSON_B64 env var
        # Use base64 encoding to avoid shell quoting issues with JSON
        if [ -n "$WEBAI_COOKIES_JSON" ]; then
          # Encode JSON as base64 to avoid shell quoting/escaping issues
          WEBAI_COOKIES_B64=$(echo -n "$WEBAI_COOKIES_JSON" | base64 -w 0)
          DOCKER_CMD="$DOCKER_CMD -e WEBAI_COOKIES_JSON_B64=$WEBAI_COOKIES_B64"
          echo "✅ WEBAI_COOKIES_JSON set (base64 encoded to avoid shell quoting issues)"
          echo "   Cookie file also written to /shared/cookies/webai_cookies.json"
          echo "   Base64 length: ${#WEBAI_COOKIES_B64} chars"
        else
          echo "⚠️  WARNING: WEBAI_COOKIES_JSON not set. WebAI Pro may not work until sidecar writes cookies."
        fi
        
        # Add Ollama and Streamlit settings
        DOCKER_CMD="$DOCKER_CMD -e OLLAMA_BASE_URL=\"${OLLAMA_BASE_URL:-http://host.docker.internal:11434}\""
        DOCKER_CMD="$DOCKER_CMD -e OLLAMA_MODEL=\"${OLLAMA_MODEL:-mistral-nemo:12b}\""
        DOCKER_CMD="$DOCKER_CMD -e OLLAMA_ENABLED=\"${OLLAMA_ENABLED:-true}\""
        DOCKER_CMD="$DOCKER_CMD -e STREAMLIT_SERVER_HEADLESS=true"
        DOCKER_CMD="$DOCKER_CMD -e STREAMLIT_BROWSER_GATHER_USAGE_STATS=false"
        DOCKER_CMD="$DOCKER_CMD -e DISABLE_SCHEDULER=true"
        DOCKER_CMD="$DOCKER_CMD trading-dashboard:latest"
        
        # Start Streamlit container in background (parallel with Flask deployment)
        eval $DOCKER_CMD &
        STREAMLIT_PID=$!
        echo "✅ Trading Dashboard Streamlit container starting (PID: $STREAMLIT_PID)"
      
      # Deploy Flask container (Trading Dashboard Flask app on port 5001)
      # Deploy in parallel with Streamlit for faster deployment
      - |
        echo "Deploying Trading Dashboard Flask container in parallel..."
        # Build base docker run command for Flask
        FLASK_CMD="docker run -d --name trading-dashboard-flask --restart unless-stopped -p 5001:5001 --network bridge-ipv6 --add-host=host.docker.internal:host-gateway"
        # Mount application logs directory (shared with Streamlit container)
        FLASK_CMD="$FLASK_CMD -v /home/lance/trading-dashboard-logs:/app/web_dashboard/logs"
        # Mount shared cookies directory
        FLASK_CMD="$FLASK_CMD -v /shared/cookies:/shared/cookies"
        FLASK_CMD="$FLASK_CMD -e SUPABASE_URL=\"$SUPABASE_URL\""
        FLASK_CMD="$FLASK_CMD -e SUPABASE_PUBLISHABLE_KEY=\"$SUPABASE_PUBLISHABLE_KEY\""
        FLASK_CMD="$FLASK_CMD -e SUPABASE_SECRET_KEY=\"$SUPABASE_SECRET_KEY\""
        FLASK_CMD="$FLASK_CMD -e BUILD_TIMESTAMP=\"$BUILD_TIMESTAMP\""
        FLASK_CMD="$FLASK_CMD -e FLASK_PORT=5001"
        
        # Add APP_DOMAIN (required for auth callbacks and cookie domain)
        [ -n "$APP_DOMAIN" ] && FLASK_CMD="$FLASK_CMD -e APP_DOMAIN=\"$APP_DOMAIN\""
        
        # Add optional environment variables if set
        [ -n "$RESEARCH_DATABASE_URL" ] && FLASK_CMD="$FLASK_CMD -e RESEARCH_DATABASE_URL=\"$RESEARCH_DATABASE_URL\""
        [ -n "$SUPABASE_DATABASE_URL" ] && FLASK_CMD="$FLASK_CMD -e SUPABASE_DATABASE_URL=\"$SUPABASE_DATABASE_URL\""
        [ -n "$FMP_API_KEY" ] && FLASK_CMD="$FLASK_CMD -e FMP_API_KEY=\"$FMP_API_KEY\""
        
        # Add WebAI cookies (same as Streamlit container)
        if [ -n "$WEBAI_COOKIES_JSON" ]; then
          WEBAI_COOKIES_B64=$(echo -n "$WEBAI_COOKIES_JSON" | base64 -w 0)
          FLASK_CMD="$FLASK_CMD -e WEBAI_COOKIES_JSON_B64=$WEBAI_COOKIES_B64"
        fi
        
        FLASK_CMD="$FLASK_CMD trading-dashboard-flask:latest"
        
        # Execute the command
        eval $FLASK_CMD
        echo "✅ Trading Dashboard Flask container deployed on port 5001"
      
      # Wait for Streamlit container to finish starting
      - |
        wait $STREAMLIT_PID 2>/dev/null || true
        echo "✅ All containers deployed successfully"
      
      # Verify cookie file is accessible in container (retry loop instead of fixed sleep)
      - |
        i=1
        while [ $i -le 5 ]; do
          if docker exec trading-dashboard test -f /shared/cookies/webai_cookies.json 2>/dev/null; then
            echo "✅ Cookie file is accessible in container"
            docker exec trading-dashboard head -c 100 /shared/cookies/webai_cookies.json
            echo "..."
            break
          fi
          if [ $i -lt 5 ]; then
            sleep 0.5
          else
            echo "⚠️  Cookie file not found in container after 5 attempts (will use base64 env var fallback)"
          fi
          i=$((i + 1))
        done
      
      # Deploy cookie refresher sidecar container (runs independently, persists across redeployments)
      - echo "Deploying cookie refresher sidecar..."
      # Write config file to shared volume (container reads this, no restart needed)
      - |
        if [ -z "$AI_SERVICE_WEB_URL" ]; then
          echo "❌ ERROR: AI_SERVICE_WEB_URL is NOT SET"
          echo "   Make sure Woodpecker secret 'ai_service_web_url' is configured"
          exit 1
        fi
        # Write config to shared volume (container will read this on next refresh cycle)
        echo "{\"AI_SERVICE_WEB_URL\": \"$AI_SERVICE_WEB_URL\"}" > /shared/cookies/ai_service_config.json
        chmod 644 /shared/cookies/ai_service_config.json
        echo "✅ Wrote AI_SERVICE_WEB_URL to shared config file"
      # Stop and restart cookie-refresher to apply new image (if container exists)
      - |
        if docker ps -a --format '{{.Names}}' | grep -q '^cookie-refresher$'; then
          echo "Stopping existing cookie-refresher container..."
          docker stop cookie-refresher 2>/dev/null || true
          docker rm cookie-refresher 2>/dev/null || true
          echo "✅ Stopped and removed old cookie-refresher container"
        fi
        
        echo "Starting cookie refresher sidecar container..."
        # Copy initial cookies from Woodpecker secret if available (first time only)
        if [ -n "$WEBAI_COOKIES_JSON" ] && [ ! -f /shared/cookies/webai_cookies.json ]; then
          echo "$WEBAI_COOKIES_JSON" > /shared/cookies/webai_cookies.json
          echo "✅ Initialized cookie file from Woodpecker secret"
        fi
        
        # Start the sidecar container with latest image
        docker run -d \
          --name cookie-refresher \
          --restart unless-stopped \
          --network bridge-ipv6 \
          -v /shared/cookies:/shared/cookies \
          -e COOKIE_REFRESH_INTERVAL=1800 \
          -e COOKIE_OUTPUT_FILE=/shared/cookies/webai_cookies.json \
          -e COOKIE_INPUT_FILE=/shared/cookies/webai_cookies.json \
          -e AI_SERVICE_WEB_URL="$AI_SERVICE_WEB_URL" \
          cookie-refresher:latest
        echo "✅ Cookie refresher sidecar started with latest image"
      
      # Deploy static files (auth callback HTML, cookie setting page, and login form)
      - echo "Deploying static files..."
      - mkdir -p /deploy_target/frontend
      - cp web_dashboard/static/auth_callback.html /deploy_target/frontend/auth_callback.html
      - cp web_dashboard/static/set_cookie.html /deploy_target/frontend/set_cookie.html
      - cp web_dashboard/static/login.html /deploy_target/frontend/login.html
      
      # Deploy Research PDF files efficiently (only copy if Research folder exists and has PDFs)
      # Only copies files that are newer than destination (incremental update)
      - |
        if [ -d "Research" ] && [ "$(find Research -name '*.pdf' -type f 2>/dev/null | wc -l)" -gt 0 ]; then
          echo "Deploying Research PDF files..."
          mkdir -p /deploy_target/Research
          # Copy PDFs preserving directory structure, only if source is newer (efficient incremental)
          find Research -name '*.pdf' -type f | while read pdf_file; do
            dest_file="/deploy_target/$pdf_file"
            dest_dir=$(dirname "$dest_file")
            mkdir -p "$dest_dir"
            # Only copy if source is newer or destination doesn't exist
            if [ ! -f "$dest_file" ] || [ "$pdf_file" -nt "$dest_file" ]; then
              cp "$pdf_file" "$dest_file"
            fi
          done
          echo "✅ Research files deployed"
        else
          echo "ℹ️  No Research folder or PDFs found, skipping..."
        fi
      
      # Clean up old Docker images (keep last 5 versions)
      # This prevents disk space issues from accumulating old image versions
      - echo "Cleaning up old Docker images (keeping last 5)..."
      - docker images -q trading-dashboard | tail -n +6 | xargs -r docker rmi > /dev/null 2>&1 || true
      - docker images -q trading-dashboard-flask | tail -n +6 | xargs -r docker rmi > /dev/null 2>&1 || true
      
      - echo "✅ Build and deployment complete!"
    when:
      event: push
      branch: [main, master]


